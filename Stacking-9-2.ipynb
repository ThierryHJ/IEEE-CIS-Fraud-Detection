{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestClassifier,  GradientBoostingClassifier,AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error,roc_auc_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import glob\n",
    "\n",
    "from scipy.stats import describe\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = pd.read_csv('train_FE.csv')\n",
    "test_X = pd.read_csv('test_FE.csv')\n",
    "train_Y = pd.read_csv('train_FE_Y_3.csv',header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb = xgb.XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    random_state=2019\n",
    "#     tree_method='gpu_hist'  # THE MAGICAL PARAMETER\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBoost = GradientBoostingClassifier(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   loss='huber', random_state =43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgb = lgb.LGBMClassifier(num_leaves= 491,\n",
    "           min_child_weight =0.03454472573214212,\n",
    "           feature_fraction = 0.3797454081646243,\n",
    "           bagging_fraction = 0.4181193142567742,\n",
    "           min_data_in_leaf = 106,\n",
    "           objective = 'binary',\n",
    "           max_depth = -1,\n",
    "           learning_rate = 0.01,\n",
    "           boosting_type='gbdt',\n",
    "          bagging_seed=11,\n",
    "          verbosity=-1,\n",
    "          reg_alpha=0.3899927210061127,\n",
    "          reg_lambda=0.6485237330340494,\n",
    "          random_state=47,num_boost_round=7000, verbose_eval=200,eval_metric='auc')\n",
    "\n",
    "def lightgbm(X_train,X_holdout,y_train,y_holdout):\n",
    "    dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "    dvalid = lgb.Dataset(X_holdout, label=y_holdout)\n",
    "    \n",
    "    params = {'num_leaves': 491,\n",
    "          'min_child_weight': 0.03454472573214212,\n",
    "          'feature_fraction': 0.3797454081646243,\n",
    "          'bagging_fraction': 0.4181193142567742,\n",
    "          'min_data_in_leaf': 106,\n",
    "          'objective': 'binary',\n",
    "          'max_depth': -1,\n",
    "          'learning_rate': 0.01,\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'auc',\n",
    "          \"verbosity\": -1,\n",
    "          'reg_alpha': 0.3899927210061127,\n",
    "          'reg_lambda': 0.6485237330340494,\n",
    "          'random_state': 47,\n",
    "         }\n",
    "    \n",
    "    clf = lgb.train(params, dtrain, 10000, valid_sets = [dtrain, dvalid], verbose_eval=200, early_stopping_rounds=500)\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_clf = AdaBoostClassifier(n_estimators=700, \n",
    "                             learning_rate=0.045,\n",
    "                             base_estimator=DecisionTreeClassifier(max_depth=5),\n",
    "                             random_state=829,\n",
    "                             algorithm='SAMME.R')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "second-stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta regresser\n",
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "class Ensemble(object):\n",
    "    def __init__(self, n_folds, stacker, base_models):\n",
    "        self.n_folds = n_folds\n",
    "        self.stacker = stacker\n",
    "        self.base_models = base_models\n",
    "\n",
    "    def fit_predict(self, X, y, T):\n",
    "#         X = np.array(X)\n",
    "#         y = np.array(y)\n",
    "#         T = np.array(T)\n",
    "\n",
    "        folds = KFold(n_splits=self.n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "        S_train = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        S_test = np.zeros((T.shape[0], len(self.base_models)))\n",
    "\n",
    "        for i, clf in enumerate(self.base_models):\n",
    "            print('Training {} model'.format(clf))\n",
    "            S_test_i = np.zeros((T.shape[0], self.n_folds))\n",
    "            \n",
    "            # for the third or later models, which are learned by sklearn, nan value is not allowed\n",
    "            if i >= 2:\n",
    "                X = X.replace([np.inf, -np.inf], np.nan)\n",
    "                y = y.replace([np.inf, -np.inf], np.nan)\n",
    "                T = T.replace([np.inf, -np.inf], np.nan)\n",
    "                \n",
    "                X = X.fillna(X.mean())\n",
    "                y = y.fillna(y.mean())\n",
    "                T = T.fillna(T.mean())\n",
    "\n",
    "            # begin the cross validation\n",
    "            for j, (train_idx, val_idx) in enumerate(folds.split(X)):\n",
    "                start_time = time.time()\n",
    "                \n",
    "                print('The fold {}'.format(j+1))\n",
    "                X_train = X.iloc[train_idx]\n",
    "                y_train = y.iloc[train_idx]\n",
    "                X_holdout = X.iloc[val_idx]\n",
    "                y_holdout = y.iloc[val_idx]\n",
    "                \n",
    "                # fit and predict\n",
    "                # if it is lightgbm model\n",
    "                if i == 0:\n",
    "                    clf = lightgbm(X_train,X_holdout,y_train,y_holdout)\n",
    "                # otherwise\n",
    "                else:\n",
    "                    clf.fit(X_train, y_train)\n",
    "                \n",
    "                if i == 0:\n",
    "                    y_pred_val = clf.predict(X_holdout)[:]\n",
    "                else:\n",
    "                    y_pred_val = clf.predict_proba(X_holdout)[:,1]\n",
    "                \n",
    "#                 # save trained model\n",
    "#                 filename = 'model_{}.sav'.format(i)\n",
    "#                 pickle.dump(clf, open(filename, 'wb'))\n",
    "  \n",
    "                # calculate auc score\n",
    "                y_pred_auc = roc_auc_score(y_holdout, y_pred_val)\n",
    "                \n",
    "                print('Fold {} | AUC: {}'.format(j+1,y_pred_auc))\n",
    "                print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "                \n",
    "                # put predicted data into holdout position\n",
    "                S_train[val_idx, i] = y_pred_val\n",
    "                \n",
    "                # put predicted data into test data\n",
    "                if i == 0:\n",
    "                    S_test_i[:, j] = clf.predict(T)[:]\n",
    "                else:\n",
    "                    S_test_i[:, j] = clf.predict_proba(T)[:,1]\n",
    "                    \n",
    "                # save memory\n",
    "                del X_train, X_holdout, y_train, y_holdout\n",
    "    \n",
    "                #gabage collector\n",
    "                gc.collect()\n",
    "                print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "                \n",
    "            \n",
    "            \n",
    "            # aggregate predicted test data \n",
    "            S_test[:, i] = S_test_i.mean(1)\n",
    "            print('pass')\n",
    "            \n",
    "            # save each model's predicted test data\n",
    "            S_test_df = pd.DataFrame(S_test)\n",
    "            S_test_df.to_csv(\"prediction_model_{}_test.csv\".format(j), index=False)\n",
    "            \n",
    "            \n",
    "            # save each model's predicted train data\n",
    "            S_train_df = pd.DataFrame(S_train)\n",
    "            S_train_df.to_csv(\"prediction_model_{}_train.csv\".format(j), index=False)\n",
    "            \n",
    "            print('End of training {} model'.format(clf))\n",
    "        \n",
    "        # second-stage\n",
    "        self.stacker.fit(S_train, y)\n",
    "        y_pred = self.stacker.predict(S_test)[:]\n",
    "        print('End of training')\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_averaged_models = Ensemble(n_folds=5, stacker=lasso, base_models=['model_lgb',model_xgb])\n",
    "\n",
    "y_pred_test_vectors = stacked_averaged_models.fit_predict(X=train_X,y=train_Y,T=test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
